{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1fLWqm9tzsg",
        "outputId": "7c9bbd2b-0c25-4079-83fd-f0bd030aea26"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fc714404650>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2giu1BwZhH8N"
      },
      "source": [
        "# Architecture of Diff Components\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQ3D553sfuuy"
      },
      "outputs": [],
      "source": [
        "class Embedding(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_dict_len, embed_size, device):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.embed = nn.Embedding(embed_dict_len, embed_size, device=device)\n",
        "\n",
        "    def forward(self, word_idxs):\n",
        "                                            # Here word_idxs should be (batch_size, seq_len)\n",
        "        embeddings = self.embed(word_idxs)  # (batch_size, seq_len, embed_size)\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "class RNNEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, num_layers, bidirectional):\n",
        "        super(RNNEncoder, self).__init__()\n",
        "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)    ### BATCH_FIRST\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        y, _ = self.rnn(x)  # (batch_size, seq_len, 2*hidden_size)\n",
        "\n",
        "        return y\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size, device):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.layer_stack = nn.Sequential(\n",
        "              nn.Linear(2*hidden_size, 2*hidden_size, device=device),\n",
        "              nn.ReLU(),\n",
        "              nn.Linear(2*hidden_size, 1, device=device)\n",
        "        )\n",
        "\n",
        "    def forward(self, q):\n",
        "        logits = self.layer_stack(q) # (bs, q_len, 1)\n",
        "        return logits\n",
        "\n",
        "class Attention(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size, device):\n",
        "        super(Attention, self).__init__()\n",
        "        self.q_attender = NeuralNet(hidden_size, device=device)\n",
        "\n",
        "    def forward(self, c, q):\n",
        "        batch_size, c_len, _ = c.shape # was size()  c and q are (bs, c_len or q_len, 2*hid_size)\n",
        "        q_len = q.size(1)\n",
        "        logits_q = self.q_attender(q)\n",
        "        alphas_q = F.softmax(logits_q, dim=1).squeeze().unsqueeze(1) # (bs, 1, q_len)\n",
        "\n",
        "        # (bs, 1, q_len) x (bs, q_len, 2*hid_size) => (bs, 1, 2*hid_size)\n",
        "        attended_q = torch.bmm(alphas_q, q)\n",
        "\n",
        "        logits_c = torch.bmm(c, attended_q.squeeze().unsqueeze(2))  # (bs, c_len, 1)\n",
        "        alphas_c = F.softmax(logits_c, dim=1).squeeze().unsqueeze(1)  # (bs, 1, c_len)\n",
        "\n",
        "        # (bs, 1, c_len) x (bs, c_len, 2*hid_size) => (bs, 1, 2*hid_size)\n",
        "        attended_c = torch.bmm(alphas_c, c)\n",
        "\n",
        "\n",
        "        attended = torch.cat([attended_q.expand(-1, c_len, -1), c, attended_c.expand(-1, c_len, -1)], dim=2) # (bs, c_len, 6*hid_size)\n",
        "\n",
        "        return attended\n",
        "\n",
        "\n",
        "\n",
        "class Output(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Output, self).__init__()\n",
        "\n",
        "        self.outRnn = RNNEncoder(input_size=6*hidden_size,\n",
        "                              hidden_size=2,\n",
        "                              num_layers=2, bidirectional=False)\n",
        "\n",
        "    def forward(self, attended):\n",
        "        out = self.outRnn(attended)   # (bs, c_len, 2)\n",
        "        logits_1, logits_2 = out[:,:,0], out[:,:,1]  # (batch_size, seq_len)\n",
        "\n",
        "        # Shapes: (batch_size, seq_len)\n",
        "        # p1 = F.softmax(logits_1.squeeze(), dim=1)\n",
        "        # p2 = F.softmax(logits_2.squeeze(), dim=1)                                 # EXPLICITLY INCLUDED THE DIM ARGUMENT: NOT CHECKED\n",
        "\n",
        "        return logits_1, logits_2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33EW-6PLhgPe"
      },
      "source": [
        "# Defining the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlTOEVkVaF7E"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    \"\"\"Follows a high-level structure commonly found in SQuAD models:\n",
        "        - Embedding layer: Embed word indices to get word vectors.\n",
        "        - Encoder layer: Encode the embedded sequence.\n",
        "        - Attention layer: Apply an attention mechanism to the encoded sequence.\n",
        "        - Output layer: Simple layer (e.g., linear + softmax) and also an LSTM RNN to get final outputs.\n",
        "    Args:\n",
        "        word_vectors (torch.Tensor): Pre-trained word vectors.                   ### Not in this case\n",
        "        hidden_size (int): Number of features in the hidden state at each layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size, embed_dict_size, embed_size, device):\n",
        "        super(Model, self).__init__()\n",
        "\n",
        "        # self.emb_q = Embedding(embed_dict_size, embed_size)\n",
        "        self.emb = Embedding(embed_dict_size, embed_size, device=device)\n",
        "\n",
        "        self.enc_q = RNNEncoder(input_size=embed_size,\n",
        "                                     hidden_size=hidden_size,\n",
        "                                     num_layers=2, bidirectional=True)\n",
        "\n",
        "        self.enc_c = RNNEncoder(input_size=embed_size,\n",
        "                                     hidden_size=hidden_size,\n",
        "                                     num_layers=2, bidirectional=True)\n",
        "        self.attender = NeuralNet(hidden_size=hidden_size, device=device)\n",
        "\n",
        "        self.att = Attention(hidden_size=hidden_size, device=device)\n",
        "\n",
        "        self.out = Output(hidden_size=hidden_size)\n",
        "\n",
        "    def forward(self, cw_idxs, qw_idxs):\n",
        "        # print(\"ccww\",cw_idxs)\n",
        "        # print(\"qqww\",qw_idxs)\n",
        "        cw_embed = self.emb(cw_idxs)  # (batch_size, c_len, embed_size)\n",
        "\n",
        "        qw_embed = self.emb(qw_idxs)  # (batch_size, q_len, embed_size)\n",
        "        # print(qw_embed)\n",
        "        c_enc = self.enc_c(cw_embed)    # (batch_size, c_len, 2*hidden_size)\n",
        "        q_enc = self.enc_q(qw_embed)     # (batch_size, q_len, 2*hidden_size)\n",
        "        # print(\"enc\",q_enc)\n",
        "        attended = self.att(c_enc, q_enc)  # (bs, c_len, 6*hidden_size)\n",
        "        # print()\n",
        "        out = self.out(attended)  # 2 tensors, each (batch_size, c_len)\n",
        "        # print(\"out\",out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JF_2AMo8BAwb",
        "outputId": "88cb5203-3e1e-49fd-8afd-6b457afad327"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHUTbaaj6CkX"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9vo1haruOs-"
      },
      "outputs": [],
      "source": [
        "# ### DATA LOADING\n",
        "\n",
        "# import os\n",
        "# import tqdm\n",
        "# import json\n",
        "# import zipfile\n",
        "# import tarfile\n",
        "# import pickle\n",
        "# import numpy as np\n",
        "# import urllib.request\n",
        "\n",
        "# filepath = '/content/gdrive/My Drive/train-v2.0.json'\n",
        "# with open(filepath) as f:\n",
        "#     data = json.load(f)\n",
        "\n",
        "# from spacy.lang.en import English\n",
        "# tokenizer = English()\n",
        "\n",
        "# def word_tokenize(sent):\n",
        "#     return [token.text for token in tokenizer(sent)]\n",
        "\n",
        "# def clean_text(text):\n",
        "#     text = text.replace(\"]\", \" ] \")\n",
        "#     text = text.replace(\"[\", \" [ \")\n",
        "#     text = text.replace(\"\\n\", \" \")\n",
        "#     text = text.replace(\"''\", '\" ').replace(\"``\", '\" ')\n",
        "\n",
        "#     return text\n",
        "\n",
        "# def convert_idx(text, tokens):\n",
        "#     current = 0\n",
        "#     spans = []\n",
        "#     for token in tokens:\n",
        "#         current = text.find(token, current)\n",
        "#         if current < 0:\n",
        "#             print(\"Token {} cannot be found\".format(token))\n",
        "#             raise Exception()\n",
        "#         spans.append((current, current + len(token)))\n",
        "#         current += len(token)\n",
        "#     return spans\n",
        "\n",
        "# # these lists contain arrays, where each array contains tokens\n",
        "# # context_file[i] will correspond to question_file[i] etc, and a token is a word\n",
        "# context_file = []\n",
        "# question_file = []\n",
        "# answer_file = []\n",
        "# labels_file = []\n",
        "\n",
        "# quesCount = {\"Who\":0,\"What\":0,\"When\":0,\"Where\":0}\n",
        "# lenCount = dict()\n",
        "\n",
        "\n",
        "# for article_id in tqdm.tqdm(range(len(data['data']))):\n",
        "#     list_paragraphs = data['data'][article_id]['paragraphs']\n",
        "#     # loop over the paragraphs\n",
        "#     for paragraph in list_paragraphs:\n",
        "#         context = paragraph['context']\n",
        "#         context = clean_text(context)\n",
        "#         context_tokens = [w for w in word_tokenize(context) if w]\n",
        "#         spans = convert_idx(context, context_tokens)\n",
        "#         qas = paragraph['qas']\n",
        "#         # loop over Q/A\n",
        "#         for qa in qas:\n",
        "#             question = qa['question']\n",
        "#             question = clean_text(question)\n",
        "#             question_tokens = [w for w in word_tokenize(question) if w]\n",
        "#             # if str(question_tokens[0]) in quesCount:\n",
        "#             #     quesCount[str(question_tokens[0])]+=1\n",
        "#             # elif len(question_tokens)>1 and str(question_tokens[1]) in quesCount:\n",
        "#             #     quesCount[str(question_tokens[1])]+=1\n",
        "#             factoid = False\n",
        "\n",
        "#             for qtype in quesCount:\n",
        "#                 if qtype in question_tokens:\n",
        "#                     factoid = True\n",
        "#                     quesCount[qtype]+=1\n",
        "#                 elif (qtype.lower()) in question_tokens:\n",
        "#                     factoid = True\n",
        "#                     quesCount[qtype]+=1\n",
        "\n",
        "#             if not factoid:\n",
        "#                 continue\n",
        "\n",
        "#             if len(question_tokens) in lenCount:\n",
        "#                 lenCount[len(question_tokens)]+=1\n",
        "#             else:\n",
        "#                 lenCount[len(question_tokens)]=1\n",
        "#             # select only one ground truth, the top answer, if any answer\n",
        "#             answer_ids = 1 if qa['answers'] else 0\n",
        "#             labels = []\n",
        "#             if answer_ids:\n",
        "#                 # for answer_id in range(answer_ids):\n",
        "#                 answer_id = 0\n",
        "#                 answer = qa['answers'][answer_id]['text']\n",
        "#                 answer = clean_text(answer)\n",
        "#                 answer_tokens = [w for w in word_tokenize(answer) if w]\n",
        "#                 if len(answer_tokens)>9:\n",
        "#                     continue\n",
        "#                 answer_start = qa['answers'][answer_id]['answer_start']\n",
        "#                 answer_stop = answer_start + len(answer)\n",
        "#                 answer_span = []\n",
        "#                 for idx, span in enumerate(spans):\n",
        "#                     if not (answer_stop <= span[0] or answer_start >= span[1]):\n",
        "#                         answer_span.append(idx)\n",
        "#                 if not answer_span:\n",
        "#                     continue\n",
        "#                 labels.append([answer_span[0], answer_span[-1]])\n",
        "#                 # labels.append(str(answer_span[0]) + ' ' + str(answer_span[-1]))\n",
        "\n",
        "#                 context_file.append([token for token in context_tokens])\n",
        "#                 question_file.append([token for token in question_tokens])\n",
        "#                 answer_file.append([token for token in answer_tokens])\n",
        "#                 labels_file.append(labels)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYMtv-NxCkIt"
      },
      "outputs": [],
      "source": [
        "# ### replace the tokenized words with their associated ID in the vocabulary\n",
        "\n",
        "# with open('/content/gdrive/My Drive/word2idx.json','rb') as e:\n",
        "#     word2idx_dict = json.load(e)\n",
        "\n",
        "\n",
        "# context_idxs = []\n",
        "# question_idxs = []\n",
        "# for i, (c, q) in tqdm.tqdm(enumerate(zip(context_file, question_file))):\n",
        "\n",
        "#     len_c, len_q = len(c), len(q)\n",
        "#     # create empty numpy arrays\n",
        "#     context_idx = np.zeros([len_c], dtype=np.int32)\n",
        "#     question_idx = np.zeros([len_q], dtype=np.int32)\n",
        "\n",
        "#     # replace 0 values with word and char IDs\n",
        "#     for j, word in enumerate(c):\n",
        "#         context_idx[j] = word2idx_dict[word]\n",
        "\n",
        "#     context_idxs.append(context_idx)\n",
        "\n",
        "#     for j, word in enumerate(q):\n",
        "#         question_idx[j] = word2idx_dict[word]\n",
        "\n",
        "#     question_idxs.append(question_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiIUgB-JaxIR"
      },
      "outputs": [],
      "source": [
        "\n",
        "#np.savez('/content/gdrive/My Drive/data.npz', context=context_idxs, question=question_idxs, label=labels_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgj9wpBye8fx"
      },
      "source": [
        "# Some essential presuppositions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mh5UzoAFe7wd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75179720-4a49-497e-9dda-a7455b1aeba5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "### Some essential hyper params\n",
        "embedding_size = 400\n",
        "hidden_size = embedding_size//2\n",
        "\n",
        "batch_size = 32\n",
        "learning_rate = 1e-4\n",
        "\n",
        "MODEL_PATH = '/content/gdrive/My Drive/qa_model.tar'\n",
        "\n",
        "\n",
        "### LOADING THE VOCAB_DICT\n",
        "import json\n",
        "\n",
        "with open('/content/gdrive/My Drive/idx2word.json','rb') as f:\n",
        "    idx2word_dict = json.load(f)\n",
        "\n",
        "embed_dict_size = len(idx2word_dict)\n",
        "\n",
        "### MODEL INSTANTIATION\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "\n",
        "model = Model(hidden_size, embed_dict_size, embedding_size, device).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJS72eW1c9KB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "data = np.load('/content/gdrive/My Drive/data.npz', allow_pickle=True)\n",
        "context_idxs = data['context']\n",
        "question_idxs = data['question']\n",
        "labels_file = data['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooEGKt_4B-II"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CustomDataset(Dataset): # inherited Dataset\n",
        "    def __init__(self, context_file, question_file, labels_file):\n",
        "        self.contexts = context_file\n",
        "        self.questions = question_file\n",
        "        #self.answers = answer_file\n",
        "        self.labels = labels_file\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        return torch.FloatTensor(self.contexts[idx]), torch.FloatTensor(self.questions[idx]), torch.FloatTensor(self.labels[idx]) ## CAN CHANGE IF WANTED\n",
        "\n",
        "\n",
        "from torch.nn.utils.rnn import *\n",
        "\n",
        "def pad_collate(batch):\n",
        "  (xx1, xx2, yy) = zip(*batch)\n",
        "  x1_lens = [len(x) for x in xx1]\n",
        "  x2_lens = [len(x) for x in xx2]\n",
        "  y_lens = [len(y) for y in yy]\n",
        "\n",
        "  xx1_pad = pad_sequence(xx1, batch_first=True, padding_value=0)\n",
        "  xx2_pad = pad_sequence(xx2, batch_first=True, padding_value=0)\n",
        "  yy_pad = pad_sequence(yy, batch_first=True, padding_value=0)\n",
        "\n",
        "  return xx1_pad, xx2_pad, yy_pad, x1_lens, x2_lens, y_lens\n",
        "\n",
        "dataset = CustomDataset(context_idxs,question_idxs,labels_file)\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3GccRcnJs3J",
        "outputId": "3ab68ce0-590d-4543-9b87-67e75794f4e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  641,   991,    43, 36779, 37201,  7564,  2019], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "question_idxs[56000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GW33qblls9VY",
        "outputId": "475b7e20-a475-43c1-ffe6-0419a2ea45c6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2010"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "len(data_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXB24fZnTMg2",
        "outputId": "4d0dd0f0-6e21-4fa6-e3a6-1fee294027e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.6656e+04, 1.7000e+02, 4.3000e+01, 3.6140e+03, 4.7000e+01, 4.3000e+01,\n",
            "         3.5582e+04, 2.3000e+01, 9.7309e+04, 1.5640e+04, 2.0190e+03, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [1.6656e+04, 1.6000e+01, 1.0323e+04, 2.8000e+01, 5.4255e+04, 8.4390e+03,\n",
            "         2.8000e+01, 2.0190e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [3.2900e+03, 4.2810e+03, 4.7000e+01, 1.6300e+02, 1.7000e+02, 7.1600e+02,\n",
            "         2.8000e+01, 3.3460e+03, 5.8610e+03, 2.0190e+03, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [6.4100e+02, 9.9100e+02, 4.5030e+03, 4.8540e+03, 1.3510e+03, 2.8000e+01,\n",
            "         7.5800e+02, 2.0190e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [6.4100e+02, 1.7000e+02, 7.6580e+03, 3.0502e+04, 3.8000e+01, 1.6558e+04,\n",
            "         2.0190e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [1.7991e+04, 4.6130e+03, 1.3551e+04, 1.6200e+02, 4.3000e+01, 3.1719e+04,\n",
            "         2.0190e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [3.2900e+03, 1.4300e+03, 1.2856e+04, 1.0450e+03, 1.6200e+02, 1.0588e+04,\n",
            "         4.8590e+03, 6.5340e+03, 4.6400e+02, 2.0190e+03, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [3.2900e+03, 9.2800e+02, 1.7000e+02, 2.5082e+04, 9.4100e+03, 9.5650e+03,\n",
            "         7.2230e+03, 1.3030e+03, 1.5400e+02, 2.0190e+03, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [1.7991e+04, 1.6000e+01, 4.3000e+01, 1.7450e+04, 2.3000e+01, 1.2490e+03,\n",
            "         2.9283e+04, 5.0880e+03, 2.0190e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [2.7190e+03, 4.3000e+01, 4.4000e+01, 4.8950e+03, 1.4000e+01, 1.7000e+03,\n",
            "         4.1870e+03, 5.8700e+02, 1.3210e+03, 4.0870e+03, 9.7940e+03, 2.0190e+03,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [2.5700e+02, 1.7000e+03, 4.8800e+02, 9.9100e+02, 4.3000e+01, 1.8000e+01,\n",
            "         1.1835e+04, 3.6440e+03, 5.6960e+03, 2.6300e+02, 3.9510e+03, 5.8500e+02,\n",
            "         2.6300e+02, 9.0230e+03, 1.5900e+02, 4.3000e+01, 1.4900e+03, 1.9323e+04,\n",
            "         4.1000e+01, 4.3000e+01, 2.2387e+04, 5.0434e+04, 6.8270e+03, 2.0190e+03],\n",
            "        [1.7991e+04, 9.9100e+02, 1.9203e+04, 3.2198e+04, 2.0190e+03, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [1.6656e+04, 2.0300e+02, 4.6304e+04, 4.2466e+04, 3.9430e+03, 2.3000e+01,\n",
            "         2.8350e+03, 4.6403e+04, 4.7400e+02, 2.0190e+03, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [2.1300e+02, 1.7000e+03, 1.5540e+03, 9.9100e+02, 2.0987e+04, 5.8360e+03,\n",
            "         1.7817e+04, 2.0190e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [3.2900e+03, 9.9100e+02, 4.3000e+01, 3.0200e+02, 2.4320e+03, 5.3380e+03,\n",
            "         1.5900e+02, 8.0500e+03, 2.8000e+01, 3.0295e+04, 1.4000e+01, 5.1530e+03,\n",
            "         4.3000e+01, 2.4856e+04, 2.4320e+03, 2.0190e+03, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [3.2900e+03, 4.2810e+03, 4.7000e+01, 5.7220e+03, 2.0300e+02, 4.3000e+01,\n",
            "         5.4790e+03, 4.7000e+01, 3.4445e+04, 1.5982e+04, 1.6590e+03, 1.6540e+03,\n",
            "         1.1400e+02, 2.0190e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [3.2900e+03, 2.4830e+03, 4.7000e+01, 3.8000e+01, 1.1352e+04, 2.0400e+02,\n",
            "         1.5181e+04, 5.4153e+04, 4.1920e+03, 2.7050e+03, 3.2800e+02, 5.6500e+03,\n",
            "         5.0100e+02, 1.4970e+03, 4.7000e+01, 2.2900e+02, 1.3880e+03, 2.0190e+03,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [3.2900e+03, 2.4830e+03, 4.7000e+01, 7.5800e+02, 1.7000e+02, 5.5900e+02,\n",
            "         4.3400e+02, 5.5000e+01, 4.3000e+01, 7.1670e+03, 2.0190e+03, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [3.2900e+03, 2.0300e+02, 1.0690e+03, 9.9992e+04, 4.7000e+01, 9.3520e+03,\n",
            "         2.0190e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [1.2200e+02, 1.5645e+04, 3.9320e+03, 4.5000e+02, 1.6000e+01, 5.8500e+02,\n",
            "         1.7000e+03, 2.0190e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [3.2900e+03, 3.7500e+02, 2.1093e+04, 2.0300e+02, 7.1600e+02, 2.8000e+01,\n",
            "         3.8000e+01, 3.3470e+03, 1.1097e+04, 2.0190e+03, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [1.2200e+02, 5.7250e+03, 9.9800e+02, 1.7616e+04, 2.0200e+02, 1.6000e+01,\n",
            "         2.5370e+03, 5.0100e+02, 1.3190e+03, 6.7060e+03, 1.6460e+03, 1.7000e+03,\n",
            "         2.0190e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [1.9859e+04, 2.0300e+02, 2.0400e+02, 7.1600e+02, 1.5400e+02, 1.7000e+03,\n",
            "         4.2810e+03, 4.7000e+01, 6.0240e+03, 2.0190e+03, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [1.7991e+04, 4.6040e+03, 3.0875e+04, 4.3900e+02, 3.3600e+02, 1.7000e+02,\n",
            "         4.6130e+03, 5.5000e+01, 4.3000e+01, 5.8120e+03, 4.7000e+01, 3.2228e+04,\n",
            "         2.0190e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [3.2900e+03, 2.3210e+03, 2.3430e+03, 2.2450e+03, 2.0370e+03, 1.6299e+04,\n",
            "         2.4200e+02, 4.7000e+01, 4.3000e+01, 2.2410e+03, 2.8000e+01, 9.4310e+03,\n",
            "         2.0190e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [3.2900e+03, 9.9100e+02, 3.1700e+02, 5.1129e+04, 7.7670e+03, 1.5900e+02,\n",
            "         2.8000e+01, 4.7700e+02, 7.3400e+02, 1.4000e+01, 4.0000e+03, 1.3531e+04,\n",
            "         5.8500e+02, 6.6000e+01, 2.0190e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [1.4785e+04, 1.6354e+04, 1.7000e+02, 6.6400e+03, 5.5000e+01, 1.7000e+03,\n",
            "         2.0190e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [3.2900e+03, 9.2800e+02, 9.9100e+02, 1.0267e+04, 3.5500e+02, 4.1000e+01,\n",
            "         4.1300e+02, 3.8000e+01, 1.7120e+03, 8.5190e+03, 5.4870e+03, 2.6235e+04,\n",
            "         2.0190e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [3.2900e+03, 1.6000e+01, 4.3000e+01, 2.8700e+02, 4.7000e+01, 6.0000e+01,\n",
            "         4.7000e+01, 4.3000e+01, 1.7790e+03, 1.0584e+04, 2.8000e+01, 7.5300e+03,\n",
            "         1.1190e+03, 2.0190e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [3.2900e+03, 4.8800e+02, 1.7000e+02, 9.8038e+04, 7.8500e+02, 3.9000e+01,\n",
            "         1.1000e+01, 2.0190e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [3.2900e+03, 1.7000e+02, 4.3000e+01, 9.6460e+03, 1.1606e+04, 6.9800e+02,\n",
            "         2.6150e+03, 2.8000e+01, 4.3000e+01, 6.1600e+02, 4.4720e+03, 6.3600e+02,\n",
            "         2.0190e+03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
            "        [3.2900e+03, 4.4650e+03, 1.7000e+03, 7.1600e+02, 4.1000e+01, 4.1850e+03,\n",
            "         4.3000e+01, 7.6240e+03, 7.1100e+03, 2.0190e+03, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]])\n",
            "torch.Size([32, 24])\n"
          ]
        }
      ],
      "source": [
        "for batch in data_loader:\n",
        "    print(batch[1])\n",
        "    print(batch[1].shape)\n",
        "    # print(batch[2].transpose(0,2)[1].transpose(0,1).squeeze())\n",
        "    #print(batch[2].squeeze().transpose(0,1)[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zw6TfqzpMbFp"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mzst84eMgx9"
      },
      "outputs": [],
      "source": [
        "# print(\"Starting training...\")\n",
        "# losses = []\n",
        "# num_epochs = 5\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     print(\"##### epoch {:2d}\".format(epoch + 1))\n",
        "#     model.train()\n",
        "#     train_losses = 0\n",
        "#     for batch in data_loader:\n",
        "#         cw_idxs, qw_idxs, label1, label2 =                          torch.Tensor(batch[0]).long().to(device), \\\n",
        "#                                                                        torch.Tensor(batch[1]).long().to(device), \\\n",
        "#                                                                        torch.Tensor(batch[2].transpose(0,2)[0].transpose(0,1).squeeze()).long().to(device), \\\n",
        "#                                                                        torch.Tensor(batch[2].transpose(0,2)[1].transpose(0,1).squeeze()).long().to(device)\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         pred1, pred2 = model(cw_idxs, qw_idxs)\n",
        "#         loss = criterion(pred1, label1) + criterion(pred2, label2)           # label1, label2 one hot? NOT NEEDED\n",
        "#         #loss = loss/batch_size\n",
        "#         train_losses += loss.item()                                           # Does loss add accross batches? YES IT TAKES THE MEAN\n",
        "\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#     losses.append(train_losses)\n",
        "#     # for name, param in model.named_parameters():\n",
        "#     #     print(f'{name}.grad', param.grad)\n",
        "#     print(\"Training loss of the model at epoch {} is: {}\".format(epoch + 1, int(train_losses)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iQ3Ge3TeHH_"
      },
      "source": [
        "# Saving the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsHHuYtPuMY1"
      },
      "outputs": [],
      "source": [
        "# torch.save({\n",
        "#             'epoch': epoch,\n",
        "#             'model_state_dict': model.state_dict(),\n",
        "#             'optimizer_state_dict': optimizer.state_dict(),\n",
        "#             'loss': loss,\n",
        "#             'losses': losses,\n",
        "#             }, MODEL_PATH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnuvtLNH84JO"
      },
      "source": [
        "# Alternative Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"Starting training...\")\n",
        "# losses = []\n",
        "# epoch = 0\n",
        "# loss = 0\n",
        "\n",
        "# torch.save({\n",
        "#             'epoch': epoch,\n",
        "#             'model_state_dict': model.state_dict(),\n",
        "#             'optimizer_state_dict': optimizer.state_dict(),\n",
        "#             'loss': loss,\n",
        "#             'losses': losses,\n",
        "#             }, MODEL_PATH)\n"
      ],
      "metadata": {
        "id": "sCg1ALB3rRQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nWev9uZ83gH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "outputId": "aa456d7f-96f4-4155-8de4-327656ddae64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuming training...\n",
            "##### epoch 39\n",
            "Training loss of the model at epoch 39 is: 16184\n",
            "##### epoch 40\n",
            "Training loss of the model at epoch 40 is: 16183\n",
            "##### epoch 41\n",
            "Training loss of the model at epoch 41 is: 16151\n",
            "##### epoch 42\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-7692eaee7031>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mtrain_losses\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                                           \u001b[0;31m# Does loss add accross batches? YES IT TAKES THE MEAN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "print(\"Resuming training...\")\n",
        "\n",
        "while True:\n",
        "    checkpoint = torch.load(MODEL_PATH)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['loss']\n",
        "    losses = checkpoint['losses']\n",
        "\n",
        "    print(\"##### epoch {:2d}\".format(epoch + 1))\n",
        "    model.train()\n",
        "    train_losses = 0\n",
        "    for batch in data_loader:\n",
        "        cw_idxs, qw_idxs, label1, label2 =                          torch.Tensor(batch[0]).long().to(device), \\\n",
        "                                                                       torch.Tensor(batch[1]).long().to(device), \\\n",
        "                                                                       torch.Tensor(batch[2].transpose(0,2)[0].transpose(0,1).squeeze()).long().to(device), \\\n",
        "                                                                       torch.Tensor(batch[2].transpose(0,2)[1].transpose(0,1).squeeze()).long().to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        pred1, pred2 = model(cw_idxs, qw_idxs)\n",
        "        loss = criterion(pred1, label1) + criterion(pred2, label2)           # label1, label2 one hot? NOT NEEDED\n",
        "        #loss = loss/batch_size\n",
        "        train_losses += loss.item()                                           # Does loss add accross batches? YES IT TAKES THE MEAN\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "    print(\"Training loss of the model at epoch {} is: {}\".format(epoch + 1, int(train_losses)))\n",
        "\n",
        "    losses.append(train_losses)\n",
        "    epoch += 1\n",
        "\n",
        "    torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            'losses': losses,\n",
        "            }, MODEL_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lU69ciVPh5bq"
      },
      "outputs": [],
      "source": [
        "# for name, param in model.named_parameters():\n",
        "#         print(f'{name}.grad', param.grad)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "o2Ke-WmCD4aH",
        "outputId": "b4893ced-0788-4cca-e603-7bceb907488d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(losses)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvDgqmT6GGbq"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bZ2XDPFGBEm"
      },
      "source": [
        "### Demo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"\"\"During the Cold War, American troops and their allies fought Communist forces in Korea and Vietnam.\n",
        "The Korean War began in 1950, when the Soviets walked out of a U.N. Security meeting, removing their possible veto.\"\"\"\n",
        "question = \"When did the american troops fight vietnamese communist forces?\"\n",
        "\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "from spacy.lang.en import English\n",
        "tokenizer = English()\n",
        "\n",
        "def word_tokenize(sent):\n",
        "    return [token.text for token in tokenizer(sent)]\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.replace(\"]\", \" ] \")\n",
        "    text = text.replace(\"[\", \" [ \")\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    text = text.replace(\"''\", '\" ').replace(\"``\", '\" ')\n",
        "\n",
        "    return text\n",
        "\n",
        "context = clean_text(context)\n",
        "context_tokens = [w for w in word_tokenize(context) if w]\n",
        "\n",
        "question = clean_text(question)\n",
        "question_tokens = [w for w in word_tokenize(question) if w]\n",
        "\n",
        "### replace the tokenized words with their associated ID in the vocabulary\n",
        "\n",
        "with open('/content/gdrive/My Drive/word2idx.json','rb') as e:\n",
        "    word2idx_dict = json.load(e)\n",
        "\n",
        "\n",
        "context_idxs = []\n",
        "question_idxs = []\n",
        "len_c, len_q = len(context_tokens), len(question_tokens)\n",
        "# create empty numpy arrays\n",
        "context_idx = np.zeros([len_c], dtype=np.int32)\n",
        "question_idx = np.zeros([len_q], dtype=np.int32)\n",
        "\n",
        "# replace 0 values with word IDs\n",
        "for j, word in enumerate(context_tokens):\n",
        "   try:\n",
        "     context_idx[j] = word2idx_dict[word]\n",
        "   except KeyError:\n",
        "     context_idx[j] = 0\n",
        "\n",
        "\n",
        "for j, word in enumerate(question_tokens):\n",
        "   try:\n",
        "     question_idx[j] = word2idx_dict[word]\n",
        "   except KeyError:\n",
        "     question_idx[j] = 0\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    pred1, pred2 = model(torch.Tensor(np.array([context_idx, context_idx])).long().to(device), torch.Tensor(np.array([question_idx, question_idx])).long().to(device))\n",
        "    start, end = torch.argmax(pred1, dim=1)[0].item(), torch.argmax(pred2, dim=1)[0].item()\n",
        "\n",
        "answer = ''\n",
        "for index in range(start, end+1):\n",
        "    answer = answer + context_tokens[index] + ' '\n",
        "\n",
        "print(answer)\n",
        "\n",
        "if answer == '':\n",
        "    print('answer not found.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MioJMX4EttQi",
        "outputId": "a7f1ccc2-1dce-443d-b845-534b5223b346"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1950 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2Tm9eefF39l",
        "outputId": "3740a089-82c2-4f4b-8a71-98f77521a744"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Delhi \n"
          ]
        }
      ],
      "source": [
        "context = \"New Delhi is the capital of India. It has a population of nearly three crore. It is a beautiful city.\"\n",
        "question = \"Which is a beautiful city?\"\n",
        "\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "from spacy.lang.en import English\n",
        "tokenizer = English()\n",
        "\n",
        "def word_tokenize(sent):\n",
        "    return [token.text for token in tokenizer(sent)]\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.replace(\"]\", \" ] \")\n",
        "    text = text.replace(\"[\", \" [ \")\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    text = text.replace(\"''\", '\" ').replace(\"``\", '\" ')\n",
        "\n",
        "    return text\n",
        "\n",
        "context = clean_text(context)\n",
        "context_tokens = [w for w in word_tokenize(context) if w]\n",
        "\n",
        "question = clean_text(question)\n",
        "question_tokens = [w for w in word_tokenize(question) if w]\n",
        "\n",
        "### replace the tokenized words with their associated ID in the vocabulary\n",
        "\n",
        "with open('/content/gdrive/My Drive/word2idx.json','rb') as e:\n",
        "    word2idx_dict = json.load(e)\n",
        "\n",
        "\n",
        "context_idxs = []\n",
        "question_idxs = []\n",
        "len_c, len_q = len(context_tokens), len(question_tokens)\n",
        "# create empty numpy arrays\n",
        "context_idx = np.zeros([len_c], dtype=np.int32)\n",
        "question_idx = np.zeros([len_q], dtype=np.int32)\n",
        "\n",
        "# replace 0 values with word IDs\n",
        "for j, word in enumerate(context_tokens):\n",
        "   try:\n",
        "     context_idx[j] = word2idx_dict[word]\n",
        "   except KeyError:\n",
        "     context_idx[j] = 0\n",
        "\n",
        "\n",
        "for j, word in enumerate(question_tokens):\n",
        "   try:\n",
        "     question_idx[j] = word2idx_dict[word]\n",
        "   except KeyError:\n",
        "     question_idx[j] = 0\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    pred1, pred2 = model(torch.Tensor(np.array([context_idx, context_idx])).long().to(device), torch.Tensor(np.array([question_idx, question_idx])).long().to(device))\n",
        "    start, end = torch.argmax(pred1, dim=1)[0].item(), torch.argmax(pred2, dim=1)[0].item()\n",
        "\n",
        "answer = ''\n",
        "for index in range(start, end+1):\n",
        "    answer = answer + context_tokens[index] + ' '\n",
        "\n",
        "print(answer)\n",
        "\n",
        "if answer == '':\n",
        "    print('answer not found.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CK-0HHNNgmow",
        "outputId": "07744411-de50-4437-9d99-fe5735510d8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kabir \n"
          ]
        }
      ],
      "source": [
        "context = 'This is Doctor Kabir Rajdheer Singh. Topper of the board, topper of the college, topper of the university, he is the one with an impeccable academic record. But with respect to anger management he is a big zero. He is not a rebel without a cause.'\n",
        "question = 'Who has anger management issues?'\n",
        "\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "from spacy.lang.en import English\n",
        "tokenizer = English()\n",
        "\n",
        "def word_tokenize(sent):\n",
        "    return [token.text for token in tokenizer(sent)]\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.replace(\"]\", \" ] \")\n",
        "    text = text.replace(\"[\", \" [ \")\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    text = text.replace(\"''\", '\" ').replace(\"``\", '\" ')\n",
        "\n",
        "    return text\n",
        "\n",
        "context = clean_text(context)\n",
        "context_tokens = [w for w in word_tokenize(context) if w]\n",
        "\n",
        "question = clean_text(question)\n",
        "question_tokens = [w for w in word_tokenize(question) if w]\n",
        "\n",
        "### replace the tokenized words with their associated ID in the vocabulary\n",
        "\n",
        "with open('/content/gdrive/My Drive/word2idx.json','rb') as e:\n",
        "    word2idx_dict = json.load(e)\n",
        "\n",
        "\n",
        "context_idxs = []\n",
        "question_idxs = []\n",
        "len_c, len_q = len(context_tokens), len(question_tokens)\n",
        "# create empty numpy arrays\n",
        "context_idx = np.zeros([len_c], dtype=np.int32)\n",
        "question_idx = np.zeros([len_q], dtype=np.int32)\n",
        "\n",
        "# replace 0 values with word IDs\n",
        "for j, word in enumerate(context_tokens):\n",
        "   try:\n",
        "     context_idx[j] = word2idx_dict[word]\n",
        "   except KeyError:\n",
        "     context_idx[j] = 0\n",
        "\n",
        "\n",
        "for j, word in enumerate(question_tokens):\n",
        "   try:\n",
        "     question_idx[j] = word2idx_dict[word]\n",
        "   except KeyError:\n",
        "     question_idx[j] = 0\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    pred1, pred2 = model(torch.Tensor(np.array([context_idx, context_idx])).long().to(device), torch.Tensor(np.array([question_idx, question_idx])).long().to(device))\n",
        "    start, end = torch.argmax(pred1, dim=1)[0].item(), torch.argmax(pred2, dim=1)[0].item()\n",
        "\n",
        "answer = ''\n",
        "for index in range(start, end+1):\n",
        "    answer = answer + context_tokens[index] + ' '\n",
        "\n",
        "print(answer)\n",
        "\n",
        "if answer == '':\n",
        "    print('answer not found.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bclGI0hSqxlv",
        "outputId": "474978a0-ef11-43a5-e82a-03800c53bb2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2020 \n"
          ]
        }
      ],
      "source": [
        "context = \"Keeley Rebecca Hazell (born 18 September 1986) is an English model and actress. Hazell was a Page 3 girl and has worked with magazines such as FHM, Loaded, Nuts and Zoo Weekly. She has also made numerous television appearances and has focused on her acting career, appearing in films such as Horrible Bosses 2 (2014) and the streaming television series Ted Lasso (2020). \"\n",
        "question = \"Who is Keeley Rebecca Hazell?\"\n",
        "\n",
        "\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "from spacy.lang.en import English\n",
        "tokenizer = English()\n",
        "\n",
        "def word_tokenize(sent):\n",
        "    return [token.text for token in tokenizer(sent)]\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.replace(\"]\", \" ] \")\n",
        "    text = text.replace(\"[\", \" [ \")\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    text = text.replace(\"''\", '\" ').replace(\"``\", '\" ')\n",
        "\n",
        "    return text\n",
        "\n",
        "context = clean_text(context)\n",
        "context_tokens = [w for w in word_tokenize(context) if w]\n",
        "\n",
        "question = clean_text(question)\n",
        "question_tokens = [w for w in word_tokenize(question) if w]\n",
        "\n",
        "### replace the tokenized words with their associated ID in the vocabulary\n",
        "\n",
        "with open('/content/gdrive/My Drive/word2idx.json','rb') as e:\n",
        "    word2idx_dict = json.load(e)\n",
        "\n",
        "\n",
        "context_idxs = []\n",
        "question_idxs = []\n",
        "len_c, len_q = len(context_tokens), len(question_tokens)\n",
        "# create empty numpy arrays\n",
        "context_idx = np.zeros([len_c], dtype=np.int32)\n",
        "question_idx = np.zeros([len_q], dtype=np.int32)\n",
        "\n",
        "# replace 0 values with word IDs\n",
        "for j, word in enumerate(context_tokens):\n",
        "   try:\n",
        "     context_idx[j] = word2idx_dict[word]\n",
        "   except KeyError:\n",
        "     context_idx[j] = 0\n",
        "\n",
        "\n",
        "for j, word in enumerate(question_tokens):\n",
        "   try:\n",
        "     question_idx[j] = word2idx_dict[word]\n",
        "   except KeyError:\n",
        "     question_idx[j] = 0\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    pred1, pred2 = model(torch.Tensor(np.array([context_idx, context_idx])).long().to(device), torch.Tensor(np.array([question_idx, question_idx])).long().to(device))\n",
        "    start, end = torch.argmax(pred1, dim=1)[0].item(), torch.argmax(pred2, dim=1)[0].item()\n",
        "\n",
        "answer = ''\n",
        "for index in range(start, end+1):\n",
        "    answer = answer + context_tokens[index] + ' '\n",
        "\n",
        "print(answer)\n",
        "\n",
        "if answer == '':\n",
        "    print('answer not found.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKgacVQKGKSZ"
      },
      "outputs": [],
      "source": [
        "index = 0\n",
        "for batch in data_loader:\n",
        "  datapoint = batch\n",
        "  break\n",
        "\n",
        "cw_idxs, qw_idxs, label1, label2 =                          torch.Tensor(batch[0]).long().to(device), \\\n",
        "                                                                       torch.Tensor(batch[1]).long().to(device), \\\n",
        "                                                                       torch.Tensor(batch[2].transpose(0,2)[0].transpose(0,1).squeeze()).long().to(device), \\\n",
        "                                                                       torch.Tensor(batch[2].transpose(0,2)[1].transpose(0,1).squeeze()).long().to(device)\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    pred1, pred2 = model(cw_idxs, qw_idxs)\n",
        "    start, end = torch.argmax(pred1[index]), torch.argmax(pred2[index])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ox4TAJvDuE6T",
        "outputId": "dcba2baf-0aaa-4d5c-bbff-75b3c2b96462"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "predicted 85 85\n",
            "actual 16 16\n"
          ]
        }
      ],
      "source": [
        "print('predicted', start.item(), end.item())\n",
        "print('actual', label1[index].item(), label2[index].item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arbCDW1YDu7D"
      },
      "source": [
        "## Picking good prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pKd1crmDuKV",
        "outputId": "72903839-1624-4dca-f509-7996d2bb171c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_overlap 5\n",
            "{'gt': [68, 72], 'pred': [48, 72]}\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "max_overlap = 0\n",
        "with torch.no_grad():\n",
        "    for batch in data_loader:\n",
        "            cw_idxs, qw_idxs, label1, label2 =                          torch.Tensor(batch[0]).long().to(device), \\\n",
        "                                                                       torch.Tensor(batch[1]).long().to(device), \\\n",
        "                                                                       torch.Tensor(batch[2].transpose(0,2)[0].transpose(0,1).squeeze()).long().to(device), \\\n",
        "                                                                       torch.Tensor(batch[2].transpose(0,2)[1].transpose(0,1).squeeze()).long().to(device)\n",
        "\n",
        "            pred1, pred2 = model(cw_idxs, qw_idxs)\n",
        "\n",
        "            start_indices, end_indices = torch.argmax(pred1, dim=1), torch.argmax(pred2, dim=1)\n",
        "\n",
        "            batch_size = cw_idxs.size(0)\n",
        "\n",
        "            for sample_num in range(batch_size):\n",
        "                prediction = [start_indices[sample_num].item(), end_indices[sample_num].item()]\n",
        "                if prediction[0] > prediction[1]:\n",
        "                  continue\n",
        "\n",
        "                gt = [label1[sample_num].item(), label2[sample_num].item()]\n",
        "                [first, last] = sorted([prediction, gt])\n",
        "                f1, f2 = first\n",
        "                l1, l2 = last\n",
        "                if l2 <= f2:\n",
        "                  overlap = l2-l1+1\n",
        "                elif l1 <= f2:\n",
        "                  overlap = f2-l1+1\n",
        "                else:\n",
        "                  continue\n",
        "\n",
        "                if overlap > max_overlap:\n",
        "                  max_overlap = overlap\n",
        "                  best = {'gt':gt, 'pred':prediction}\n",
        "            break\n",
        "\n",
        "print('max_overlap',max_overlap)\n",
        "print(best)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U43Ci6PIrXrc"
      },
      "source": [
        "# Evaluation using Metrics: Precision, Recall and F-score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYM8dosKo-wn"
      },
      "source": [
        "## Generating Test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQN0M416o4WD",
        "outputId": "741c842a-d10f-45e4-b296-6d60788c96c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 35/35 [00:04<00:00,  8.41it/s]\n"
          ]
        }
      ],
      "source": [
        "### DATA LOADING\n",
        "\n",
        "import os\n",
        "import tqdm\n",
        "import json\n",
        "import zipfile\n",
        "import tarfile\n",
        "import pickle\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "\n",
        "filepath = '/content/gdrive/My Drive/dev-v2.0.json'\n",
        "with open(filepath) as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "from spacy.lang.en import English\n",
        "tokenizer = English()\n",
        "\n",
        "def word_tokenize(sent):\n",
        "    return [token.text for token in tokenizer(sent)]\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.replace(\"]\", \" ] \")\n",
        "    text = text.replace(\"[\", \" [ \")\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    text = text.replace(\"''\", '\" ').replace(\"``\", '\" ')\n",
        "\n",
        "    return text\n",
        "\n",
        "def convert_idx(text, tokens):\n",
        "    current = 0\n",
        "    spans = []\n",
        "    for token in tokens:\n",
        "        current = text.find(token, current)\n",
        "        if current < 0:\n",
        "            print(\"Token {} cannot be found\".format(token))\n",
        "            raise Exception()\n",
        "        spans.append((current, current + len(token)))\n",
        "        current += len(token)\n",
        "    return spans\n",
        "\n",
        "# these lists contain arrays, where each array contains tokens\n",
        "# context_file[i] will correspond to question_file[i] etc, and a token is a word\n",
        "context_file = []\n",
        "question_file = []\n",
        "answer_file = []\n",
        "labels_file = []\n",
        "\n",
        "quesCount = {\"Who\":0,\"What\":0,\"When\":0,\"Where\":0}\n",
        "lenCount = dict()\n",
        "\n",
        "\n",
        "for article_id in tqdm.tqdm(range(len(data['data']))):\n",
        "    list_paragraphs = data['data'][article_id]['paragraphs']\n",
        "    # loop over the paragraphs\n",
        "    for paragraph in list_paragraphs:\n",
        "        context = paragraph['context']\n",
        "        context = clean_text(context)\n",
        "        context_tokens = [w for w in word_tokenize(context) if w]\n",
        "        spans = convert_idx(context, context_tokens)\n",
        "        qas = paragraph['qas']\n",
        "        # loop over Q/A\n",
        "        for qa in qas:\n",
        "            question = qa['question']\n",
        "            question = clean_text(question)\n",
        "            question_tokens = [w for w in word_tokenize(question) if w]\n",
        "            # if str(question_tokens[0]) in quesCount:\n",
        "            #     quesCount[str(question_tokens[0])]+=1\n",
        "            # elif len(question_tokens)>1 and str(question_tokens[1]) in quesCount:\n",
        "            #     quesCount[str(question_tokens[1])]+=1\n",
        "            factoid = False\n",
        "\n",
        "            for qtype in quesCount:\n",
        "                if qtype in question_tokens:\n",
        "                    factoid = True\n",
        "                    quesCount[qtype]+=1\n",
        "                elif (qtype.lower()) in question_tokens:\n",
        "                    factoid = True\n",
        "                    quesCount[qtype]+=1\n",
        "\n",
        "            if not factoid:\n",
        "                continue\n",
        "\n",
        "            if len(question_tokens) in lenCount:\n",
        "                lenCount[len(question_tokens)]+=1\n",
        "            else:\n",
        "                lenCount[len(question_tokens)]=1\n",
        "            # select only one ground truth, the top answer, if any answer\n",
        "            answer_ids = 1 if qa['answers'] else 0\n",
        "            labels = []\n",
        "            if answer_ids:\n",
        "                # for answer_id in range(answer_ids):\n",
        "                answer_id = 0\n",
        "                answer = qa['answers'][answer_id]['text']\n",
        "                answer = clean_text(answer)\n",
        "                answer_tokens = [w for w in word_tokenize(answer) if w]\n",
        "                if len(answer_tokens)>9:\n",
        "                    continue\n",
        "                answer_start = qa['answers'][answer_id]['answer_start']\n",
        "                answer_stop = answer_start + len(answer)\n",
        "                answer_span = []\n",
        "                for idx, span in enumerate(spans):\n",
        "                    if not (answer_stop <= span[0] or answer_start >= span[1]):\n",
        "                        answer_span.append(idx)\n",
        "                if not answer_span:\n",
        "                    continue\n",
        "                labels.append([answer_span[0], answer_span[-1]])\n",
        "                # labels.append(str(answer_span[0]) + ' ' + str(answer_span[-1]))\n",
        "\n",
        "                context_file.append([token for token in context_tokens])\n",
        "                question_file.append([token for token in question_tokens])\n",
        "                answer_file.append([token for token in answer_tokens])\n",
        "                labels_file.append(labels)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvRySjXiqUOd",
        "outputId": "d29ddc62-6269-4782-fe6b-d943cefee33e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "4483it [00:00, 16370.66it/s]\n"
          ]
        }
      ],
      "source": [
        "### replace the tokenized words with their associated ID in the vocabulary\n",
        "\n",
        "with open('/content/gdrive/My Drive/word2idx.json','rb') as e:\n",
        "    word2idx_dict = json.load(e)\n",
        "\n",
        "\n",
        "context_idxs = []\n",
        "question_idxs = []\n",
        "for i, (c, q) in tqdm.tqdm(enumerate(zip(context_file, question_file))):\n",
        "\n",
        "    len_c, len_q = len(c), len(q)\n",
        "    # create empty numpy arrays\n",
        "    context_idx = np.zeros([len_c], dtype=np.int32)\n",
        "    question_idx = np.zeros([len_q], dtype=np.int32)\n",
        "\n",
        "    # replace 0 values with word IDs\n",
        "    for j, word in enumerate(c):\n",
        "      try:\n",
        "        context_idx[j] = word2idx_dict[word]\n",
        "      except KeyError:\n",
        "        context_idx[j] = 0\n",
        "\n",
        "    context_idxs.append(context_idx)\n",
        "\n",
        "    for j, word in enumerate(q):\n",
        "      try:\n",
        "        question_idx[j] = word2idx_dict[word]\n",
        "      except KeyError:\n",
        "        question_idx[j] = 0\n",
        "\n",
        "    question_idxs.append(question_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1rgzloLrpWR"
      },
      "outputs": [],
      "source": [
        "valid_dataset = CustomDataset(context_idxs,question_idxs,labels_file)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_collate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXutxRKvo5zA"
      },
      "source": [
        "## Evaluation Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPfMkDnkv4K2",
        "outputId": "e395dad7-f9fd-41dc-85da-c0bc84da3a67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total precision in batch 1 is 5.25\n",
            "total precision in batch 2 is 1.1592261904761905\n",
            "total precision in batch 3 is 4.295238095238096\n",
            "total precision in batch 4 is 3.0272727272727273\n",
            "total precision in batch 5 is 3.2788018433179724\n",
            "total precision in batch 6 is 3.2882809795777086\n",
            "total precision in batch 7 is 0.48621994443912253\n",
            "total precision in batch 8 is 3.763003663003663\n",
            "total precision in batch 9 is 3.399101307189542\n",
            "total precision in batch 10 is 3.151414768806073\n",
            "total precision in batch 11 is 2.0357142857142856\n",
            "total precision in batch 12 is 2.13468992248062\n",
            "total precision in batch 13 is 5.111610486891386\n",
            "total precision in batch 14 is 3.231909090909091\n",
            "total precision in batch 15 is 5.008264462809917\n",
            "total precision in batch 16 is 4.838974017321785\n",
            "total precision in batch 17 is 2.336890243902439\n",
            "total precision in batch 18 is 4.125510567863509\n",
            "total precision in batch 19 is 2.8666666666666667\n",
            "total precision in batch 20 is 2.421590909090909\n",
            "total precision in batch 21 is 7.040064102564102\n",
            "total precision in batch 22 is 2.085304054054054\n",
            "total precision in batch 23 is 3.19714790268914\n",
            "total precision in batch 24 is 1.1267361111111112\n",
            "total precision in batch 25 is 1.4088050314465408\n",
            "total precision in batch 26 is 5.12\n",
            "total precision in batch 27 is 10.030303030303031\n",
            "total precision in batch 28 is 2.129850746268657\n",
            "total precision in batch 29 is 4.059435722768143\n",
            "total precision in batch 30 is 5.0\n",
            "total precision in batch 31 is 2.342949906353688\n",
            "total precision in batch 32 is 4.207019171304886\n",
            "total precision in batch 33 is 1.3997821350762525\n",
            "total precision in batch 34 is 1.8198910043570238\n",
            "total precision in batch 35 is 1.594065934065934\n",
            "total precision in batch 36 is 1.025937081659973\n",
            "total precision in batch 37 is 5.596811594202899\n",
            "total precision in batch 38 is 1.2606060606060607\n",
            "total precision in batch 39 is 1.1630367796074284\n",
            "total precision in batch 40 is 5.116683054736153\n",
            "total precision in batch 41 is 6.146320346320347\n",
            "total precision in batch 42 is 3.9285714285714284\n",
            "total precision in batch 43 is 4.193075307045895\n",
            "total precision in batch 44 is 2.1246246246246248\n",
            "total precision in batch 45 is 3.2098401598401596\n",
            "total precision in batch 46 is 5.017857142857142\n",
            "total precision in batch 47 is 1.2992072170585018\n",
            "total precision in batch 48 is 3.356084656084656\n",
            "total precision in batch 49 is 7.0330858960763525\n",
            "total precision in batch 50 is 3.3333333333333335\n",
            "total precision in batch 51 is 7.295454545454546\n",
            "total precision in batch 52 is 3.1447619047619044\n",
            "total precision in batch 53 is 1.1979752530933634\n",
            "total precision in batch 54 is 4.0717479242069405\n",
            "total precision in batch 55 is 2.0273972602739727\n",
            "total precision in batch 56 is 5.3678160919540225\n",
            "total precision in batch 57 is 3.834390247433726\n",
            "total precision in batch 58 is 5.0704654895666135\n",
            "total precision in batch 59 is 5.0\n",
            "total precision in batch 60 is 1.1470588235294117\n",
            "total precision in batch 61 is 3.235294117647059\n",
            "total precision in batch 62 is 5.117293906810035\n",
            "total precision in batch 63 is 2.4242424242424243\n",
            "total precision in batch 64 is 2.0227272727272725\n",
            "total precision in batch 65 is 1.1567094305221874\n",
            "total precision in batch 66 is 5.0\n",
            "total precision in batch 67 is 3.2883333333333336\n",
            "total precision in batch 68 is 4.111809923130679\n",
            "total precision in batch 69 is 3.0104166666666665\n",
            "total precision in batch 70 is 6.093595679012346\n",
            "total precision in batch 71 is 2.166666666666667\n",
            "total precision in batch 72 is 1.4404552590266875\n",
            "total precision in batch 73 is 5.206208425720621\n",
            "total precision in batch 74 is 2.3712606837606836\n",
            "total precision in batch 75 is 0.07417379368598881\n",
            "total precision in batch 76 is 5.014925373134329\n",
            "total precision in batch 77 is 7.0\n",
            "total precision in batch 78 is 1.8116760828625234\n",
            "total precision in batch 79 is 3.105859958318975\n",
            "total precision in batch 80 is 2.0434782608695654\n",
            "total precision in batch 81 is 4.25\n",
            "total precision in batch 82 is 3.0\n",
            "total precision in batch 83 is 3.514619883040936\n",
            "total precision in batch 84 is 2.1818181818181817\n",
            "total precision in batch 85 is 1.0649224806201552\n",
            "total precision in batch 86 is 4.0\n",
            "total precision in batch 87 is 5.0380760460306515\n",
            "total precision in batch 88 is 2.142857142857143\n",
            "total precision in batch 89 is 3.115543799568594\n",
            "total precision in batch 90 is 3.4837662337662336\n",
            "total precision in batch 91 is 5.669162826420891\n",
            "total precision in batch 92 is 3.2861552028218695\n",
            "total precision in batch 93 is 4.510752688172043\n",
            "total precision in batch 94 is 2.0\n",
            "total precision in batch 95 is 4.0569930069930065\n",
            "total precision in batch 96 is 4.542857142857143\n",
            "total precision in batch 97 is 5.28030303030303\n",
            "total precision in batch 98 is 5.006944444444445\n",
            "total precision in batch 99 is 4.0\n",
            "total precision in batch 100 is 3.2936507936507935\n",
            "total precision in batch 101 is 1.224537037037037\n",
            "total precision in batch 102 is 5.6373106060606055\n",
            "total precision in batch 103 is 5.248081841432224\n",
            "total precision in batch 104 is 2.0869480474297295\n",
            "total precision in batch 105 is 3.955821205821206\n",
            "total precision in batch 106 is 1.0\n",
            "total precision in batch 107 is 3.1223400494874514\n",
            "total precision in batch 108 is 5.05\n",
            "total precision in batch 109 is 7.166033717834959\n",
            "total precision in batch 110 is 5.016393442622951\n",
            "total precision in batch 111 is 1.037037037037037\n",
            "total precision in batch 112 is 7.166666666666666\n",
            "total precision in batch 113 is 5.424242424242424\n",
            "total precision in batch 114 is 5.04\n",
            "total precision in batch 115 is 3.053139623925806\n",
            "total precision in batch 116 is 3.0\n",
            "total precision in batch 117 is 4.012195121951219\n",
            "total precision in batch 118 is 4.181818181818182\n",
            "total precision in batch 119 is 3.1895378151260503\n",
            "total precision in batch 120 is 2.3150454788657036\n",
            "total precision in batch 121 is 2.2677628000208645\n",
            "total precision in batch 122 is 2.4285714285714284\n",
            "total precision in batch 123 is 3.175840821858521\n",
            "total precision in batch 124 is 8.025641025641026\n",
            "total precision in batch 125 is 4.1144655172413795\n",
            "total precision in batch 126 is 4.216393442622951\n",
            "total precision in batch 127 is 7.3180555555555555\n",
            "total precision in batch 128 is 2.0885301168714414\n",
            "total precision in batch 129 is 6.068344173152789\n",
            "total precision in batch 130 is 2.054343874954562\n",
            "total precision in batch 131 is 5.239921722113503\n",
            "total precision in batch 132 is 4.135111111111111\n",
            "total precision in batch 133 is 0.20099859572476206\n",
            "total precision in batch 134 is 2.3130341880341883\n",
            "total precision in batch 135 is 4.5\n",
            "total precision in batch 136 is 4.163829787234043\n",
            "total precision in batch 137 is 2.0\n",
            "total precision in batch 138 is 3.078125\n",
            "total precision in batch 139 is 4.087853773584905\n",
            "total precision in batch 140 is 2.0144927536231885\n",
            "total precision in batch 141 is 0\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "total_precision = 0\n",
        "total_recall = 0\n",
        "total_f_score = 0\n",
        "n_samples = 0\n",
        "i = 0\n",
        "with torch.no_grad():\n",
        "    for batch in valid_dataloader:\n",
        "            cw_idxs, qw_idxs, label1, label2 =                          torch.Tensor(batch[0]).long().to(device), \\\n",
        "                                                                       torch.Tensor(batch[1]).long().to(device), \\\n",
        "                                                                       torch.Tensor(batch[2].transpose(0,2)[0].transpose(0,1).squeeze()).long().to(device), \\\n",
        "                                                                       torch.Tensor(batch[2].transpose(0,2)[1].transpose(0,1).squeeze()).long().to(device)\n",
        "\n",
        "            pred1, pred2 = model(cw_idxs, qw_idxs)\n",
        "\n",
        "            start_indices, end_indices = torch.argmax(pred1, dim=1), torch.argmax(pred2, dim=1)\n",
        "\n",
        "            batch_precision = batch_recall = batch_f_score = 0\n",
        "            batch_size = cw_idxs.size(0)\n",
        "\n",
        "            for sample_num in range(batch_size):\n",
        "                prediction = [start_indices[sample_num].item(), end_indices[sample_num].item()]\n",
        "                if prediction[0] > prediction[1]:\n",
        "                  continue\n",
        "\n",
        "                gt = [label1[sample_num].item(), label2[sample_num].item()]\n",
        "                [first, last] = sorted([prediction, gt])\n",
        "                f1, f2 = first\n",
        "                l1, l2 = last\n",
        "                if l2 <= f2:\n",
        "                  overlap = l2-l1+1\n",
        "                elif l1 <= f2:\n",
        "                  overlap = f2-l1+1\n",
        "                else:\n",
        "                  continue\n",
        "\n",
        "                precision = overlap/(prediction[1]-prediction[0]+1)\n",
        "                recall = overlap/(gt[1]-gt[0]+1)\n",
        "                f_score = 2*precision*recall/(precision+recall)\n",
        "\n",
        "                batch_precision += precision\n",
        "                batch_recall += recall\n",
        "                batch_f_score += f_score\n",
        "\n",
        "\n",
        "            total_precision += batch_precision\n",
        "            total_recall += batch_recall\n",
        "            total_f_score += batch_f_score\n",
        "\n",
        "            n_samples += batch_size\n",
        "            i += 1\n",
        "            print('total precision in batch',i,'is',batch_precision)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKWrMf4jKaIp",
        "outputId": "d45ebc0a-04af-45ee-b1f5-bba72d6d992c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The average precision is 0.11159622828351828\n",
            "The average recall is 0.11562074007980767\n",
            "The average f_score is 0.07942003416285226\n"
          ]
        }
      ],
      "source": [
        "print('The average precision is', total_precision/n_samples)\n",
        "print('The average recall is', total_recall/n_samples)\n",
        "print('The average f_score is', total_f_score/n_samples)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}